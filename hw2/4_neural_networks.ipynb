{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMNIST(data.Dataset):\n",
    "    def __init__(self, dataset_path, transform=None):\n",
    "        self.dataset_path = dataset_path\n",
    "        data = pd.read_csv(dataset_path)\n",
    "        self.xs = data.values[:, 1:].reshape(-1, 1, 28, 28)\n",
    "        self.ys = data.values[:, 0]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.xs[index], self.ys[index]\n",
    "        x = torch.FloatTensor(x)\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "        y = torch.LongTensor([y])[0]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetNotMNIST(data.Dataset):\n",
    "    def __init__(self, dataset_path, transform=None):\n",
    "        self.xs_paths = []\n",
    "        self.ys = []\n",
    "        for lable_num, label in enumerate(os.listdir(dataset_path)):\n",
    "            lable_dir = os.path.join(dataset_path, label)\n",
    "            for root, _, files in os.walk(lable_dir):\n",
    "                self.ys += [lable_num] * len(files)\n",
    "                self.xs_paths += list(map(lambda file: os.path.abspath(os.path.join(root, file)), files))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_path, y = self.xs_paths[index], self.ys[index]\n",
    "        x = io.imread(x_path).reshape(28, 28)\n",
    "        x = torch.FloatTensor([x, x, x])\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "        y = torch.LongTensor([y])[0]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Normalize([128.0], [255.0])\n",
    "mnist = DatasetMNIST(os.path.join(\"..\", \"datasets\", \"mnist.csv\"), transform)\n",
    "not_mnist = DatasetNotMNIST(os.path.join(\"..\", \"datasets\", \"notMNIST_small\"), transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, val_part=0.2):\n",
    "    n = len(dataset)\n",
    "    n_val = int(n * val_part)\n",
    "    n_train = n - n_val\n",
    "    return torch.utils.data.random_split(dataset, [n_train, n_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = dict(zip([\"train\", \"val\"], split_dataset(mnist)))\n",
    "not_mnist = dict(zip([\"train\", \"val\"], split_dataset(not_mnist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device, num_epochs=50):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    accs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            for inputs, labels in dataloader[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_acc += torch.sum(preds == labels.data).item()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader[phase].dataset)\n",
    "            epoch_acc = running_acc / len(dataloader[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'val':\n",
    "                accs.append(epoch_acc)\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMNIST(nn.Module):\n",
    "    def __init__(self, function):\n",
    "        super(ModelMNIST, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3)\n",
    "        self.conv2 = nn.Conv2d(8, 8, 3)\n",
    "        self.conv3 = nn.Conv2d(8, 8, 3)\n",
    "        self.fc1 = nn.Linear(8 * 22 * 22, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        if function is \"sigmoid\":\n",
    "            self.function = nn.Sigmoid()\n",
    "        elif function is \"tanh\":\n",
    "            self.function = nn.Tanh()\n",
    "        else:\n",
    "            self.function = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(-1, 8 * 22 * 22)\n",
    "        x = self.function(self.fc1(x))\n",
    "        x = self.function(self.fc2(x))\n",
    "        x = self.function(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_loader = {x: data.DataLoader(mnist[x], \n",
    "                                   batch_size=64,\n",
    "                                   shuffle=(x == \"train\"), \n",
    "                                   num_workers=4)\n",
    "                for x in ['train', 'val']}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: MNIST\n",
      "Activation function: sigmoid\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 2.3032 Acc: 0.1020\n",
      "val Loss: 2.3011 Acc: 0.1170\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 2.3013 Acc: 0.1126\n",
      "val Loss: 2.3008 Acc: 0.1170\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 2.3006 Acc: 0.1126\n",
      "val Loss: 2.3003 Acc: 0.1170\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 2.2995 Acc: 0.1126\n",
      "val Loss: 2.2984 Acc: 0.1170\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 2.2956 Acc: 0.1126\n",
      "val Loss: 2.2915 Acc: 0.1170\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 2.2780 Acc: 0.1335\n",
      "val Loss: 2.2598 Acc: 0.2390\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 2.2370 Acc: 0.3513\n",
      "val Loss: 2.2126 Acc: 0.4820\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 2.1813 Acc: 0.5614\n",
      "val Loss: 2.1466 Acc: 0.6310\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 2.1065 Acc: 0.6683\n",
      "val Loss: 2.0658 Acc: 0.6490\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 2.0249 Acc: 0.6985\n",
      "val Loss: 1.9881 Acc: 0.7345\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 1.9532 Acc: 0.7619\n",
      "val Loss: 1.9242 Acc: 0.7910\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 1.8975 Acc: 0.8056\n",
      "val Loss: 1.8758 Acc: 0.8035\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 1.8548 Acc: 0.8254\n",
      "val Loss: 1.8408 Acc: 0.8165\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 1.8226 Acc: 0.8386\n",
      "val Loss: 1.8181 Acc: 0.8235\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 1.7965 Acc: 0.8462\n",
      "val Loss: 1.7888 Acc: 0.8405\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 1.7731 Acc: 0.8536\n",
      "val Loss: 1.7672 Acc: 0.8515\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 1.7537 Acc: 0.8615\n",
      "val Loss: 1.7496 Acc: 0.8500\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 1.7364 Acc: 0.8650\n",
      "val Loss: 1.7389 Acc: 0.8490\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 1.7206 Acc: 0.8664\n",
      "val Loss: 1.7219 Acc: 0.8535\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 1.7059 Acc: 0.8701\n",
      "val Loss: 1.7084 Acc: 0.8630\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 1.6929 Acc: 0.8752\n",
      "val Loss: 1.6964 Acc: 0.8575\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 1.6810 Acc: 0.8799\n",
      "val Loss: 1.6862 Acc: 0.8695\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 1.6695 Acc: 0.8818\n",
      "val Loss: 1.6768 Acc: 0.8655\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 1.6598 Acc: 0.8820\n",
      "val Loss: 1.6677 Acc: 0.8650\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 1.6507 Acc: 0.8856\n",
      "val Loss: 1.6598 Acc: 0.8655\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 1.6429 Acc: 0.8880\n",
      "val Loss: 1.6534 Acc: 0.8710\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 1.6344 Acc: 0.8886\n",
      "val Loss: 1.6470 Acc: 0.8735\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 1.6284 Acc: 0.8939\n",
      "val Loss: 1.6401 Acc: 0.8785\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 1.6212 Acc: 0.8961\n",
      "val Loss: 1.6362 Acc: 0.8750\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 1.6160 Acc: 0.8965\n",
      "val Loss: 1.6284 Acc: 0.8815\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 1.6108 Acc: 0.9006\n",
      "val Loss: 1.6246 Acc: 0.8895\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 1.6057 Acc: 0.9045\n",
      "val Loss: 1.6214 Acc: 0.8820\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 1.6020 Acc: 0.9046\n",
      "val Loss: 1.6188 Acc: 0.8865\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 1.5983 Acc: 0.9052\n",
      "val Loss: 1.6163 Acc: 0.8835\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 1.5953 Acc: 0.9085\n",
      "val Loss: 1.6136 Acc: 0.8835\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 1.5908 Acc: 0.9076\n",
      "val Loss: 1.6109 Acc: 0.8840\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 1.5886 Acc: 0.9055\n",
      "val Loss: 1.6080 Acc: 0.8815\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 1.5852 Acc: 0.9071\n",
      "val Loss: 1.6055 Acc: 0.8835\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 1.5828 Acc: 0.9095\n",
      "val Loss: 1.6027 Acc: 0.8840\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 1.5801 Acc: 0.9087\n",
      "val Loss: 1.6016 Acc: 0.8845\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 1.5778 Acc: 0.9080\n",
      "val Loss: 1.6004 Acc: 0.8775\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 1.5758 Acc: 0.9026\n",
      "val Loss: 1.5990 Acc: 0.8770\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 1.5739 Acc: 0.9070\n",
      "val Loss: 1.5969 Acc: 0.8770\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 1.5718 Acc: 0.9050\n",
      "val Loss: 1.5953 Acc: 0.8645\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 1.5695 Acc: 0.9048\n",
      "val Loss: 1.5935 Acc: 0.8715\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 1.5679 Acc: 0.9025\n",
      "val Loss: 1.5913 Acc: 0.8675\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 1.5658 Acc: 0.9056\n",
      "val Loss: 1.5908 Acc: 0.8660\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 1.5644 Acc: 0.9032\n",
      "val Loss: 1.5894 Acc: 0.8710\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 1.5627 Acc: 0.9019\n",
      "val Loss: 1.5892 Acc: 0.8660\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 1.5614 Acc: 0.9005\n",
      "val Loss: 1.5874 Acc: 0.8650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "function = \"sigmoid\"\n",
    "\n",
    "print(\"Dataset: MNIST\")\n",
    "print(\"Activation function:\", function)\n",
    "\n",
    "model = ModelMNIST(function)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "model, accs[function] = train(model, mnist_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: MNIST\n",
      "Activation function: tanh\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 1.6356 Acc: 0.6466\n",
      "val Loss: 1.1551 Acc: 0.8860\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 1.0681 Acc: 0.9018\n",
      "val Loss: 1.0199 Acc: 0.9140\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.9848 Acc: 0.9261\n",
      "val Loss: 0.9787 Acc: 0.9200\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.9496 Acc: 0.9324\n",
      "val Loss: 0.9625 Acc: 0.9295\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.9271 Acc: 0.9411\n",
      "val Loss: 0.9499 Acc: 0.9305\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.9049 Acc: 0.9541\n",
      "val Loss: 0.9370 Acc: 0.9345\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.8921 Acc: 0.9589\n",
      "val Loss: 0.9342 Acc: 0.9355\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.8807 Acc: 0.9650\n",
      "val Loss: 0.9312 Acc: 0.9345\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.8722 Acc: 0.9673\n",
      "val Loss: 0.9250 Acc: 0.9390\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.8647 Acc: 0.9695\n",
      "val Loss: 0.9210 Acc: 0.9395\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.8564 Acc: 0.9749\n",
      "val Loss: 0.9196 Acc: 0.9375\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.8526 Acc: 0.9766\n",
      "val Loss: 0.9216 Acc: 0.9380\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.8453 Acc: 0.9778\n",
      "val Loss: 0.9173 Acc: 0.9395\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.8406 Acc: 0.9810\n",
      "val Loss: 0.9160 Acc: 0.9405\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.8369 Acc: 0.9832\n",
      "val Loss: 0.9117 Acc: 0.9455\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.8350 Acc: 0.9835\n",
      "val Loss: 0.9116 Acc: 0.9440\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.8323 Acc: 0.9849\n",
      "val Loss: 0.9109 Acc: 0.9440\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.8291 Acc: 0.9864\n",
      "val Loss: 0.9090 Acc: 0.9455\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.8271 Acc: 0.9881\n",
      "val Loss: 0.9099 Acc: 0.9430\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.8255 Acc: 0.9888\n",
      "val Loss: 0.9121 Acc: 0.9445\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.8226 Acc: 0.9889\n",
      "val Loss: 0.9106 Acc: 0.9435\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.8199 Acc: 0.9894\n",
      "val Loss: 0.9081 Acc: 0.9450\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.8185 Acc: 0.9901\n",
      "val Loss: 0.9067 Acc: 0.9450\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.8170 Acc: 0.9905\n",
      "val Loss: 0.9062 Acc: 0.9470\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.8159 Acc: 0.9906\n",
      "val Loss: 0.9073 Acc: 0.9460\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.8149 Acc: 0.9911\n",
      "val Loss: 0.9077 Acc: 0.9460\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.8142 Acc: 0.9914\n",
      "val Loss: 0.9073 Acc: 0.9485\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.8136 Acc: 0.9914\n",
      "val Loss: 0.9075 Acc: 0.9475\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.8131 Acc: 0.9916\n",
      "val Loss: 0.9089 Acc: 0.9455\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.8126 Acc: 0.9918\n",
      "val Loss: 0.9089 Acc: 0.9460\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.8122 Acc: 0.9919\n",
      "val Loss: 0.9093 Acc: 0.9455\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.8119 Acc: 0.9921\n",
      "val Loss: 0.9091 Acc: 0.9460\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.8116 Acc: 0.9925\n",
      "val Loss: 0.9098 Acc: 0.9450\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.8111 Acc: 0.9928\n",
      "val Loss: 0.9085 Acc: 0.9455\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.8108 Acc: 0.9926\n",
      "val Loss: 0.9092 Acc: 0.9460\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.8106 Acc: 0.9926\n",
      "val Loss: 0.9092 Acc: 0.9455\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.8103 Acc: 0.9929\n",
      "val Loss: 0.9097 Acc: 0.9465\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.8101 Acc: 0.9930\n",
      "val Loss: 0.9084 Acc: 0.9455\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.8099 Acc: 0.9930\n",
      "val Loss: 0.9087 Acc: 0.9455\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.8097 Acc: 0.9930\n",
      "val Loss: 0.9091 Acc: 0.9450\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.8095 Acc: 0.9932\n",
      "val Loss: 0.9093 Acc: 0.9450\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.8092 Acc: 0.9934\n",
      "val Loss: 0.9085 Acc: 0.9445\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.8090 Acc: 0.9935\n",
      "val Loss: 0.9092 Acc: 0.9455\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.8088 Acc: 0.9936\n",
      "val Loss: 0.9087 Acc: 0.9450\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.8086 Acc: 0.9938\n",
      "val Loss: 0.9083 Acc: 0.9445\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.8084 Acc: 0.9939\n",
      "val Loss: 0.9083 Acc: 0.9435\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.8083 Acc: 0.9939\n",
      "val Loss: 0.9078 Acc: 0.9455\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.8082 Acc: 0.9940\n",
      "val Loss: 0.9081 Acc: 0.9445\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.8080 Acc: 0.9940\n",
      "val Loss: 0.9085 Acc: 0.9440\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.8079 Acc: 0.9939\n",
      "val Loss: 0.9080 Acc: 0.9445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "function = \"tanh\"\n",
    "\n",
    "print(\"Dataset: MNIST\")\n",
    "print(\"Activation function:\", function)\n",
    "\n",
    "model = ModelMNIST(function)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "model, accs[function] = train(model, mnist_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: MNIST\n",
      "Activation function: relu\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 1.7980 Acc: 0.4074\n",
      "val Loss: 1.0224 Acc: 0.7290\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.6184 Acc: 0.8157\n",
      "val Loss: 0.3195 Acc: 0.9085\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.2264 Acc: 0.9329\n",
      "val Loss: 0.2327 Acc: 0.9325\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.1496 Acc: 0.9519\n",
      "val Loss: 0.2037 Acc: 0.9430\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.1236 Acc: 0.9593\n",
      "val Loss: 0.1913 Acc: 0.9500\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.0939 Acc: 0.9709\n",
      "val Loss: 0.2204 Acc: 0.9390\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.0694 Acc: 0.9776\n",
      "val Loss: 0.1894 Acc: 0.9480\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.0519 Acc: 0.9836\n",
      "val Loss: 0.2014 Acc: 0.9470\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.0586 Acc: 0.9795\n",
      "val Loss: 0.2107 Acc: 0.9460\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.0373 Acc: 0.9881\n",
      "val Loss: 0.1731 Acc: 0.9585\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.0269 Acc: 0.9916\n",
      "val Loss: 0.1888 Acc: 0.9550\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.0438 Acc: 0.9860\n",
      "val Loss: 0.2161 Acc: 0.9450\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.0183 Acc: 0.9934\n",
      "val Loss: 0.2206 Acc: 0.9535\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.0178 Acc: 0.9942\n",
      "val Loss: 0.2112 Acc: 0.9565\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.0101 Acc: 0.9969\n",
      "val Loss: 0.2146 Acc: 0.9560\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.0058 Acc: 0.9984\n",
      "val Loss: 0.2444 Acc: 0.9570\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.0116 Acc: 0.9966\n",
      "val Loss: 0.2457 Acc: 0.9535\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.0109 Acc: 0.9970\n",
      "val Loss: 0.2644 Acc: 0.9520\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.0066 Acc: 0.9986\n",
      "val Loss: 0.2347 Acc: 0.9565\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9995\n",
      "val Loss: 0.2349 Acc: 0.9615\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.0114 Acc: 0.9970\n",
      "val Loss: 0.2591 Acc: 0.9550\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.0244 Acc: 0.9931\n",
      "val Loss: 0.3169 Acc: 0.9480\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.0246 Acc: 0.9918\n",
      "val Loss: 0.2500 Acc: 0.9505\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.0067 Acc: 0.9984\n",
      "val Loss: 0.2273 Acc: 0.9625\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.0035 Acc: 0.9991\n",
      "val Loss: 0.2450 Acc: 0.9575\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.0010 Acc: 0.9999\n",
      "val Loss: 0.2493 Acc: 0.9610\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9999\n",
      "val Loss: 0.2523 Acc: 0.9605\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.2554 Acc: 0.9605\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.2579 Acc: 0.9605\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.2609 Acc: 0.9605\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.2633 Acc: 0.9605\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.2656 Acc: 0.9605\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2677 Acc: 0.9605\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2698 Acc: 0.9605\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2716 Acc: 0.9600\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2734 Acc: 0.9605\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2750 Acc: 0.9605\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2765 Acc: 0.9605\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2779 Acc: 0.9600\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2792 Acc: 0.9600\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2806 Acc: 0.9600\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2819 Acc: 0.9600\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2831 Acc: 0.9600\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2843 Acc: 0.9595\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2854 Acc: 0.9600\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2865 Acc: 0.9600\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2876 Acc: 0.9600\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2885 Acc: 0.9600\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2894 Acc: 0.9600\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2904 Acc: 0.9600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "function = \"relu\"\n",
    "\n",
    "print(\"Dataset: MNIST\")\n",
    "print(\"Activation function:\", function)\n",
    "\n",
    "model = ModelMNIST(function)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "model, accs[function] = train(model, mnist_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = list(range(50))\n",
    "plot1, = plt.plot(xs, accs[\"sigmoid\"])\n",
    "plot2, = plt.plot(xs, accs[\"tanh\"])\n",
    "plot3, = plt.plot(xs, accs[\"relu\"])\n",
    "\n",
    "plt.title(\"MNIST functions comparation\")\n",
    "plt.legend([plot1, plot2, plot3], [\"Sigmoid\", \"Tanh\", \"ReLU\"])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"val accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "train Loss: 0.4701 Acc: 0.8726\n",
      "val Loss: 0.2138 Acc: 0.9346\n",
      "\n",
      "Epoch 1/99\n",
      "----------\n",
      "train Loss: 0.2966 Acc: 0.9166\n",
      "val Loss: 0.1871 Acc: 0.9420\n",
      "\n",
      "Epoch 2/99\n",
      "----------\n",
      "train Loss: 0.2145 Acc: 0.9335\n",
      "val Loss: 0.1677 Acc: 0.9495\n",
      "\n",
      "Epoch 3/99\n",
      "----------\n",
      "train Loss: 0.1988 Acc: 0.9408\n",
      "val Loss: 0.1875 Acc: 0.9460\n",
      "\n",
      "Epoch 4/99\n",
      "----------\n",
      "train Loss: 0.1784 Acc: 0.9477\n",
      "val Loss: 0.1510 Acc: 0.9541\n",
      "\n",
      "Epoch 5/99\n",
      "----------\n",
      "train Loss: 0.1422 Acc: 0.9554\n",
      "val Loss: 0.1651 Acc: 0.9525\n",
      "\n",
      "Epoch 6/99\n",
      "----------\n",
      "train Loss: 0.1274 Acc: 0.9591\n",
      "val Loss: 0.1696 Acc: 0.9535\n",
      "\n",
      "Epoch 7/99\n",
      "----------\n",
      "train Loss: 0.1473 Acc: 0.9555\n",
      "val Loss: 0.1437 Acc: 0.9570\n",
      "\n",
      "Epoch 8/99\n",
      "----------\n",
      "train Loss: 0.1133 Acc: 0.9651\n",
      "val Loss: 0.1517 Acc: 0.9549\n",
      "\n",
      "Epoch 9/99\n",
      "----------\n",
      "train Loss: 0.1554 Acc: 0.9554\n",
      "val Loss: 0.1690 Acc: 0.9511\n",
      "\n",
      "Epoch 10/99\n",
      "----------\n",
      "train Loss: 0.1131 Acc: 0.9641\n",
      "val Loss: 0.1431 Acc: 0.9562\n",
      "\n",
      "Epoch 11/99\n",
      "----------\n",
      "train Loss: 0.0983 Acc: 0.9694\n",
      "val Loss: 0.1501 Acc: 0.9573\n",
      "\n",
      "Epoch 12/99\n",
      "----------\n",
      "train Loss: 0.0627 Acc: 0.9794\n",
      "val Loss: 0.1501 Acc: 0.9586\n",
      "\n",
      "Epoch 13/99\n",
      "----------\n",
      "train Loss: 0.0956 Acc: 0.9718\n",
      "val Loss: 0.1509 Acc: 0.9575\n",
      "\n",
      "Epoch 14/99\n",
      "----------\n",
      "train Loss: 0.0655 Acc: 0.9802\n",
      "val Loss: 0.1623 Acc: 0.9567\n",
      "\n",
      "Epoch 15/99\n",
      "----------\n",
      "train Loss: 0.0521 Acc: 0.9838\n",
      "val Loss: 0.1767 Acc: 0.9551\n",
      "\n",
      "Epoch 16/99\n",
      "----------\n",
      "train Loss: 0.0766 Acc: 0.9768\n",
      "val Loss: 0.1516 Acc: 0.9589\n",
      "\n",
      "Epoch 17/99\n",
      "----------\n",
      "train Loss: 0.0484 Acc: 0.9849\n",
      "val Loss: 0.1801 Acc: 0.9554\n",
      "\n",
      "Epoch 18/99\n",
      "----------\n",
      "train Loss: 0.0448 Acc: 0.9862\n",
      "val Loss: 0.2008 Acc: 0.9525\n",
      "\n",
      "Epoch 19/99\n",
      "----------\n",
      "train Loss: 0.0508 Acc: 0.9843\n",
      "val Loss: 0.1499 Acc: 0.9578\n",
      "\n",
      "Epoch 20/99\n",
      "----------\n",
      "train Loss: 0.0421 Acc: 0.9879\n",
      "val Loss: 0.1567 Acc: 0.9557\n",
      "\n",
      "Epoch 21/99\n",
      "----------\n",
      "train Loss: 0.0481 Acc: 0.9853\n",
      "val Loss: 0.1945 Acc: 0.9570\n",
      "\n",
      "Epoch 22/99\n",
      "----------\n",
      "train Loss: 0.0609 Acc: 0.9812\n",
      "val Loss: 0.1606 Acc: 0.9631\n",
      "\n",
      "Epoch 23/99\n",
      "----------\n",
      "train Loss: 0.0508 Acc: 0.9850\n",
      "val Loss: 0.1948 Acc: 0.9557\n",
      "\n",
      "Epoch 24/99\n",
      "----------\n",
      "train Loss: 0.0598 Acc: 0.9828\n",
      "val Loss: 0.1563 Acc: 0.9573\n",
      "\n",
      "Epoch 25/99\n",
      "----------\n",
      "train Loss: 0.0370 Acc: 0.9883\n",
      "val Loss: 0.1911 Acc: 0.9583\n",
      "\n",
      "Epoch 26/99\n",
      "----------\n",
      "train Loss: 0.0193 Acc: 0.9941\n",
      "val Loss: 0.1894 Acc: 0.9575\n",
      "\n",
      "Epoch 27/99\n",
      "----------\n",
      "train Loss: 0.0349 Acc: 0.9903\n",
      "val Loss: 0.2128 Acc: 0.9586\n",
      "\n",
      "Epoch 28/99\n",
      "----------\n",
      "train Loss: 0.0626 Acc: 0.9822\n",
      "val Loss: 0.1684 Acc: 0.9594\n",
      "\n",
      "Epoch 29/99\n",
      "----------\n",
      "train Loss: 0.0439 Acc: 0.9869\n",
      "val Loss: 0.1779 Acc: 0.9597\n",
      "\n",
      "Epoch 30/99\n",
      "----------\n",
      "train Loss: 0.0299 Acc: 0.9909\n",
      "val Loss: 0.2288 Acc: 0.9551\n",
      "\n",
      "Epoch 31/99\n",
      "----------\n",
      "train Loss: 0.0174 Acc: 0.9945\n",
      "val Loss: 0.2022 Acc: 0.9589\n",
      "\n",
      "Epoch 32/99\n",
      "----------\n",
      "train Loss: 0.0369 Acc: 0.9887\n",
      "val Loss: 0.1837 Acc: 0.9583\n",
      "\n",
      "Epoch 33/99\n",
      "----------\n",
      "train Loss: 0.0405 Acc: 0.9876\n",
      "val Loss: 0.1777 Acc: 0.9626\n",
      "\n",
      "Epoch 34/99\n",
      "----------\n",
      "train Loss: 0.0139 Acc: 0.9955\n",
      "val Loss: 0.1785 Acc: 0.9607\n",
      "\n",
      "Epoch 35/99\n",
      "----------\n",
      "train Loss: 0.0371 Acc: 0.9898\n",
      "val Loss: 0.1955 Acc: 0.9565\n",
      "\n",
      "Epoch 36/99\n",
      "----------\n",
      "train Loss: 0.0220 Acc: 0.9934\n",
      "val Loss: 0.2541 Acc: 0.9519\n",
      "\n",
      "Epoch 37/99\n",
      "----------\n",
      "train Loss: 0.0247 Acc: 0.9927\n",
      "val Loss: 0.1982 Acc: 0.9567\n",
      "\n",
      "Epoch 38/99\n",
      "----------\n",
      "train Loss: 0.0183 Acc: 0.9942\n",
      "val Loss: 0.1937 Acc: 0.9578\n",
      "\n",
      "Epoch 39/99\n",
      "----------\n",
      "train Loss: 0.0181 Acc: 0.9943\n",
      "val Loss: 0.1965 Acc: 0.9573\n",
      "\n",
      "Epoch 40/99\n",
      "----------\n",
      "train Loss: 0.0221 Acc: 0.9934\n",
      "val Loss: 0.1883 Acc: 0.9546\n",
      "\n",
      "Epoch 41/99\n",
      "----------\n",
      "train Loss: 0.0166 Acc: 0.9947\n",
      "val Loss: 0.1989 Acc: 0.9605\n",
      "\n",
      "Epoch 42/99\n",
      "----------\n",
      "train Loss: 0.0195 Acc: 0.9941\n",
      "val Loss: 0.2382 Acc: 0.9543\n",
      "\n",
      "Epoch 43/99\n",
      "----------\n",
      "train Loss: 0.0315 Acc: 0.9898\n",
      "val Loss: 0.2174 Acc: 0.9573\n",
      "\n",
      "Epoch 44/99\n",
      "----------\n",
      "train Loss: 0.0235 Acc: 0.9925\n",
      "val Loss: 0.2113 Acc: 0.9575\n",
      "\n",
      "Epoch 45/99\n",
      "----------\n",
      "train Loss: 0.0206 Acc: 0.9944\n",
      "val Loss: 0.2118 Acc: 0.9562\n",
      "\n",
      "Epoch 46/99\n",
      "----------\n",
      "train Loss: 0.0273 Acc: 0.9925\n",
      "val Loss: 0.2101 Acc: 0.9597\n",
      "\n",
      "Epoch 47/99\n",
      "----------\n",
      "train Loss: 0.0166 Acc: 0.9955\n",
      "val Loss: 0.2241 Acc: 0.9557\n",
      "\n",
      "Epoch 48/99\n",
      "----------\n",
      "train Loss: 0.0256 Acc: 0.9932\n",
      "val Loss: 0.1784 Acc: 0.9621\n",
      "\n",
      "Epoch 49/99\n",
      "----------\n",
      "train Loss: 0.0163 Acc: 0.9947\n",
      "val Loss: 0.1920 Acc: 0.9591\n",
      "\n",
      "Epoch 50/99\n",
      "----------\n",
      "train Loss: 0.0157 Acc: 0.9950\n",
      "val Loss: 0.1893 Acc: 0.9602\n",
      "\n",
      "Epoch 51/99\n",
      "----------\n",
      "train Loss: 0.0213 Acc: 0.9934\n",
      "val Loss: 0.1944 Acc: 0.9570\n",
      "\n",
      "Epoch 52/99\n",
      "----------\n",
      "train Loss: 0.0259 Acc: 0.9926\n",
      "val Loss: 0.2087 Acc: 0.9567\n",
      "\n",
      "Epoch 53/99\n",
      "----------\n",
      "train Loss: 0.0255 Acc: 0.9917\n",
      "val Loss: 0.1872 Acc: 0.9581\n",
      "\n",
      "Epoch 54/99\n",
      "----------\n",
      "train Loss: 0.0152 Acc: 0.9957\n",
      "val Loss: 0.2071 Acc: 0.9562\n",
      "\n",
      "Epoch 55/99\n",
      "----------\n",
      "train Loss: 0.0212 Acc: 0.9939\n",
      "val Loss: 0.2172 Acc: 0.9567\n",
      "\n",
      "Epoch 56/99\n",
      "----------\n",
      "train Loss: 0.0120 Acc: 0.9963\n",
      "val Loss: 0.2097 Acc: 0.9597\n",
      "\n",
      "Epoch 57/99\n",
      "----------\n",
      "train Loss: 0.0182 Acc: 0.9937\n",
      "val Loss: 0.1946 Acc: 0.9607\n",
      "\n",
      "Epoch 58/99\n",
      "----------\n",
      "train Loss: 0.0343 Acc: 0.9913\n",
      "val Loss: 0.1903 Acc: 0.9570\n",
      "\n",
      "Epoch 59/99\n",
      "----------\n",
      "train Loss: 0.0110 Acc: 0.9966\n",
      "val Loss: 0.1963 Acc: 0.9597\n",
      "\n",
      "Epoch 60/99\n",
      "----------\n",
      "train Loss: 0.0093 Acc: 0.9975\n",
      "val Loss: 0.2065 Acc: 0.9626\n",
      "\n",
      "Epoch 61/99\n",
      "----------\n",
      "train Loss: 0.0235 Acc: 0.9931\n",
      "val Loss: 0.2070 Acc: 0.9559\n",
      "\n",
      "Epoch 62/99\n",
      "----------\n",
      "train Loss: 0.0187 Acc: 0.9935\n",
      "val Loss: 0.2128 Acc: 0.9573\n",
      "\n",
      "Epoch 63/99\n",
      "----------\n",
      "train Loss: 0.0176 Acc: 0.9947\n",
      "val Loss: 0.1822 Acc: 0.9575\n",
      "\n",
      "Epoch 64/99\n",
      "----------\n",
      "train Loss: 0.0098 Acc: 0.9969\n",
      "val Loss: 0.1920 Acc: 0.9615\n",
      "\n",
      "Epoch 65/99\n",
      "----------\n",
      "train Loss: 0.0075 Acc: 0.9977\n",
      "val Loss: 0.2258 Acc: 0.9578\n",
      "\n",
      "Epoch 66/99\n",
      "----------\n",
      "train Loss: 0.0269 Acc: 0.9918\n",
      "val Loss: 0.1884 Acc: 0.9583\n",
      "\n",
      "Epoch 67/99\n",
      "----------\n",
      "train Loss: 0.0099 Acc: 0.9972\n",
      "val Loss: 0.2051 Acc: 0.9597\n",
      "\n",
      "Epoch 68/99\n",
      "----------\n",
      "train Loss: 0.0200 Acc: 0.9936\n",
      "val Loss: 0.1890 Acc: 0.9618\n",
      "\n",
      "Epoch 69/99\n",
      "----------\n",
      "train Loss: 0.0105 Acc: 0.9968\n",
      "val Loss: 0.1989 Acc: 0.9597\n",
      "\n",
      "Epoch 70/99\n",
      "----------\n",
      "train Loss: 0.0076 Acc: 0.9977\n",
      "val Loss: 0.1921 Acc: 0.9626\n",
      "\n",
      "Epoch 71/99\n",
      "----------\n",
      "train Loss: 0.0154 Acc: 0.9957\n",
      "val Loss: 0.2305 Acc: 0.9517\n",
      "\n",
      "Epoch 72/99\n",
      "----------\n",
      "train Loss: 0.0228 Acc: 0.9933\n",
      "val Loss: 0.2019 Acc: 0.9581\n",
      "\n",
      "Epoch 73/99\n",
      "----------\n",
      "train Loss: 0.0155 Acc: 0.9950\n",
      "val Loss: 0.2078 Acc: 0.9573\n",
      "\n",
      "Epoch 74/99\n",
      "----------\n",
      "train Loss: 0.0103 Acc: 0.9966\n",
      "val Loss: 0.2085 Acc: 0.9594\n",
      "\n",
      "Epoch 75/99\n",
      "----------\n",
      "train Loss: 0.0132 Acc: 0.9960\n",
      "val Loss: 0.2166 Acc: 0.9559\n",
      "\n",
      "Epoch 76/99\n",
      "----------\n",
      "train Loss: 0.0253 Acc: 0.9917\n",
      "val Loss: 0.2007 Acc: 0.9589\n",
      "\n",
      "Epoch 77/99\n",
      "----------\n",
      "train Loss: 0.0098 Acc: 0.9973\n",
      "val Loss: 0.1956 Acc: 0.9605\n",
      "\n",
      "Epoch 78/99\n",
      "----------\n",
      "train Loss: 0.0097 Acc: 0.9971\n",
      "val Loss: 0.1894 Acc: 0.9605\n",
      "\n",
      "Epoch 79/99\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.9983\n",
      "val Loss: 0.1990 Acc: 0.9597\n",
      "\n",
      "Epoch 80/99\n",
      "----------\n",
      "train Loss: 0.0141 Acc: 0.9957\n",
      "val Loss: 0.2545 Acc: 0.9525\n",
      "\n",
      "Epoch 81/99\n",
      "----------\n",
      "train Loss: 0.0228 Acc: 0.9931\n",
      "val Loss: 0.2153 Acc: 0.9549\n",
      "\n",
      "Epoch 82/99\n",
      "----------\n",
      "train Loss: 0.0121 Acc: 0.9969\n",
      "val Loss: 0.1880 Acc: 0.9618\n",
      "\n",
      "Epoch 83/99\n",
      "----------\n",
      "train Loss: 0.0135 Acc: 0.9961\n",
      "val Loss: 0.2135 Acc: 0.9586\n",
      "\n",
      "Epoch 84/99\n",
      "----------\n",
      "train Loss: 0.0084 Acc: 0.9965\n",
      "val Loss: 0.2142 Acc: 0.9618\n",
      "\n",
      "Epoch 85/99\n",
      "----------\n",
      "train Loss: 0.0216 Acc: 0.9938\n",
      "val Loss: 0.1847 Acc: 0.9618\n",
      "\n",
      "Epoch 86/99\n",
      "----------\n",
      "train Loss: 0.0172 Acc: 0.9945\n",
      "val Loss: 0.2025 Acc: 0.9589\n",
      "\n",
      "Epoch 87/99\n",
      "----------\n",
      "train Loss: 0.0072 Acc: 0.9977\n",
      "val Loss: 0.2097 Acc: 0.9589\n",
      "\n",
      "Epoch 88/99\n",
      "----------\n",
      "train Loss: 0.0159 Acc: 0.9951\n",
      "val Loss: 0.2102 Acc: 0.9589\n",
      "\n",
      "Epoch 89/99\n",
      "----------\n",
      "train Loss: 0.0127 Acc: 0.9961\n",
      "val Loss: 0.2325 Acc: 0.9581\n",
      "\n",
      "Epoch 90/99\n",
      "----------\n",
      "train Loss: 0.0145 Acc: 0.9951\n",
      "val Loss: 0.2444 Acc: 0.9573\n",
      "\n",
      "Epoch 91/99\n",
      "----------\n",
      "train Loss: 0.0166 Acc: 0.9954\n",
      "val Loss: 0.2200 Acc: 0.9559\n",
      "\n",
      "Epoch 92/99\n",
      "----------\n",
      "train Loss: 0.0111 Acc: 0.9968\n",
      "val Loss: 0.2419 Acc: 0.9554\n",
      "\n",
      "Epoch 93/99\n",
      "----------\n",
      "train Loss: 0.0109 Acc: 0.9962\n",
      "val Loss: 0.2223 Acc: 0.9575\n",
      "\n",
      "Epoch 94/99\n",
      "----------\n",
      "train Loss: 0.0071 Acc: 0.9976\n",
      "val Loss: 0.2259 Acc: 0.9589\n",
      "\n",
      "Epoch 95/99\n",
      "----------\n",
      "train Loss: 0.0071 Acc: 0.9979\n",
      "val Loss: 0.2417 Acc: 0.9565\n",
      "\n",
      "Epoch 96/99\n",
      "----------\n",
      "train Loss: 0.0270 Acc: 0.9925\n",
      "val Loss: 0.2133 Acc: 0.9597\n",
      "\n",
      "Epoch 97/99\n",
      "----------\n",
      "train Loss: 0.0099 Acc: 0.9967\n",
      "val Loss: 0.2029 Acc: 0.9599\n",
      "\n",
      "Epoch 98/99\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0163 Acc: 0.9950\n",
      "val Loss: 0.2240 Acc: 0.9546\n",
      "\n",
      "Epoch 99/99\n",
      "----------\n",
      "train Loss: 0.0168 Acc: 0.9948\n",
      "val Loss: 0.2233 Acc: 0.9599\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_collate_fn(add_noise=False):\n",
    "    def collate_fn(data):\n",
    "        xs, ys = zip(*data)\n",
    "        xs = torch.stack(xs, 0)\n",
    "        ys = torch.stack(ys, 0)\n",
    "        if add_noise:\n",
    "            xs += torch.randn_like(xs) / 10\n",
    "        return xs, ys\n",
    "    return collate_fn\n",
    "\n",
    "not_mnist_loader = {x: data.DataLoader(not_mnist[x], \n",
    "                                       batch_size=32,\n",
    "                                       shuffle=(x == \"train\"), \n",
    "                                       num_workers=4,\n",
    "                                       collate_fn=get_collate_fn(x == \"train\"))\n",
    "                    for x in ['train', 'val']}\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), weight_decay=1e-5)\n",
    "\n",
    "model, notMNIST_accs = train(model, not_mnist_loader, criterion, optimizer, device, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max accuracy for notMNIST dataset: 0.9631410256410257\n"
     ]
    }
   ],
   "source": [
    "print(\"Max accuracy for notMNIST dataset:\", max(notMNIST_accs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
