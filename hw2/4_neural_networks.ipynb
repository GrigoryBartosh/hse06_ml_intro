{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMNIST(data.Dataset):\n",
    "    def __init__(self, dataset_path, transform=None):\n",
    "        self.dataset_path = dataset_path\n",
    "        data = pd.read_csv(dataset_path)\n",
    "        self.xs = data.values[:, 1:].reshape(-1, 1, 28, 28)\n",
    "        self.ys = data.values[:, 0]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.xs[index], self.ys[index]\n",
    "        x = torch.FloatTensor(x)\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "        y = torch.LongTensor([y])[0]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetNotMNIST(data.Dataset):\n",
    "    def __init__(self, dataset_path, transform=None):\n",
    "        self.xs_paths = []\n",
    "        self.ys = []\n",
    "        for lable_num, label in enumerate(os.listdir(dataset_path)):\n",
    "            lable_dir = os.path.join(dataset_path, label)\n",
    "            for root, _, files in os.walk(lable_dir):\n",
    "                self.ys += [lable_num] * len(files)\n",
    "                self.xs_paths += list(map(lambda file: os.path.abspath(os.path.join(root, file)), files))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_path, y = self.xs_paths[index], self.ys[index]\n",
    "        x = io.imread(x_path).reshape(28, 28)\n",
    "        x = torch.FloatTensor([x, x, x])\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "        y = torch.LongTensor([y])[0]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Normalize([128.0], [255.0])\n",
    "mnist = DatasetMNIST(os.path.join(\"..\", \"datasets\", \"mnist.csv\"), transform)\n",
    "not_mnist = DatasetNotMNIST(os.path.join(\"..\", \"datasets\", \"notMNIST_small\"), transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, val_part=0.2):\n",
    "    n = len(dataset)\n",
    "    n_val = int(n * val_part)\n",
    "    n_train = n - n_val\n",
    "    return torch.utils.data.random_split(dataset, [n_train, n_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = dict(zip([\"train\", \"val\"], split_dataset(mnist)))\n",
    "not_mnist = dict(zip([\"train\", \"val\"], split_dataset(not_mnist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device, num_epochs=50):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    accs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            for inputs, labels in dataloader[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_acc += torch.sum(preds == labels.data).item()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader[phase].dataset)\n",
    "            epoch_acc = running_acc / len(dataloader[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'val':\n",
    "                accs.append(epoch_acc)\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMNIST(nn.Module):\n",
    "    def __init__(self, function):\n",
    "        super(ModelMNIST, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3)\n",
    "        self.conv2 = nn.Conv2d(8, 8, 3)\n",
    "        self.conv3 = nn.Conv2d(8, 8, 3)\n",
    "        self.fc1 = nn.Linear(8 * 22 * 22, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        if function is \"sigmoid\":\n",
    "            self.function = nn.Sigmoid()\n",
    "        elif function is \"tanh\":\n",
    "            self.function = nn.Tanh()\n",
    "        else:\n",
    "            self.function = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(-1, 8 * 22 * 22)\n",
    "        x = self.function(self.fc1(x))\n",
    "        x = self.function(self.fc2(x))\n",
    "        x = self.function(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_loader = {x: data.DataLoader(mnist[x], \n",
    "                                   batch_size=64,\n",
    "                                   shuffle=(x == \"train\"), \n",
    "                                   num_workers=4)\n",
    "                for x in ['train', 'val']}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: MNIST\n",
      "Activation function: sigmoid\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 2.3031 Acc: 0.1029\n",
      "val Loss: 2.3009 Acc: 0.1135\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 2.3014 Acc: 0.1120\n",
      "val Loss: 2.3005 Acc: 0.1135\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 2.3013 Acc: 0.1135\n",
      "val Loss: 2.3002 Acc: 0.1135\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 2.3009 Acc: 0.1135\n",
      "val Loss: 2.3001 Acc: 0.1135\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 2.3003 Acc: 0.1135\n",
      "val Loss: 2.2993 Acc: 0.1135\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 2.2994 Acc: 0.1135\n",
      "val Loss: 2.2980 Acc: 0.1135\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 2.2967 Acc: 0.1135\n",
      "val Loss: 2.2929 Acc: 0.1135\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 2.2844 Acc: 0.1180\n",
      "val Loss: 2.2697 Acc: 0.1565\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 2.2458 Acc: 0.3946\n",
      "val Loss: 2.2192 Acc: 0.5625\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 2.1851 Acc: 0.5910\n",
      "val Loss: 2.1478 Acc: 0.5900\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 2.1044 Acc: 0.6149\n",
      "val Loss: 2.0623 Acc: 0.6570\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 2.0215 Acc: 0.6666\n",
      "val Loss: 1.9867 Acc: 0.6375\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 1.9541 Acc: 0.6793\n",
      "val Loss: 1.9285 Acc: 0.7025\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 1.9048 Acc: 0.6995\n",
      "val Loss: 1.8878 Acc: 0.7340\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 1.8667 Acc: 0.7420\n",
      "val Loss: 1.8552 Acc: 0.7455\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 1.8376 Acc: 0.7718\n",
      "val Loss: 1.8288 Acc: 0.7530\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 1.8130 Acc: 0.7742\n",
      "val Loss: 1.8070 Acc: 0.7685\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 1.7914 Acc: 0.7920\n",
      "val Loss: 1.7891 Acc: 0.7775\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 1.7732 Acc: 0.8001\n",
      "val Loss: 1.7723 Acc: 0.7910\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 1.7573 Acc: 0.8067\n",
      "val Loss: 1.7586 Acc: 0.7915\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 1.7440 Acc: 0.8093\n",
      "val Loss: 1.7454 Acc: 0.7990\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 1.7314 Acc: 0.8100\n",
      "val Loss: 1.7340 Acc: 0.7945\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 1.7187 Acc: 0.8123\n",
      "val Loss: 1.7219 Acc: 0.7980\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 1.7072 Acc: 0.8147\n",
      "val Loss: 1.7118 Acc: 0.7985\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 1.6962 Acc: 0.8183\n",
      "val Loss: 1.7037 Acc: 0.7975\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 1.6862 Acc: 0.8226\n",
      "val Loss: 1.6933 Acc: 0.8050\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 1.6765 Acc: 0.8276\n",
      "val Loss: 1.6832 Acc: 0.8080\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 1.6677 Acc: 0.8316\n",
      "val Loss: 1.6764 Acc: 0.8120\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 1.6598 Acc: 0.8431\n",
      "val Loss: 1.6691 Acc: 0.8210\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 1.6522 Acc: 0.8509\n",
      "val Loss: 1.6609 Acc: 0.8290\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 1.6456 Acc: 0.8569\n",
      "val Loss: 1.6567 Acc: 0.8300\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 1.6394 Acc: 0.8629\n",
      "val Loss: 1.6508 Acc: 0.8420\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 1.6321 Acc: 0.8709\n",
      "val Loss: 1.6461 Acc: 0.8560\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 1.6263 Acc: 0.8722\n",
      "val Loss: 1.6390 Acc: 0.8565\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 1.6193 Acc: 0.8735\n",
      "val Loss: 1.6332 Acc: 0.8585\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 1.6139 Acc: 0.8688\n",
      "val Loss: 1.6294 Acc: 0.8530\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 1.6092 Acc: 0.8685\n",
      "val Loss: 1.6250 Acc: 0.8535\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 1.6044 Acc: 0.8706\n",
      "val Loss: 1.6220 Acc: 0.8455\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 1.6005 Acc: 0.8700\n",
      "val Loss: 1.6177 Acc: 0.8450\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 1.5960 Acc: 0.8731\n",
      "val Loss: 1.6148 Acc: 0.8465\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 1.5934 Acc: 0.8710\n",
      "val Loss: 1.6134 Acc: 0.8415\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 1.5905 Acc: 0.8722\n",
      "val Loss: 1.6094 Acc: 0.8425\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 1.5874 Acc: 0.8708\n",
      "val Loss: 1.6061 Acc: 0.8410\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 1.5849 Acc: 0.8700\n",
      "val Loss: 1.6038 Acc: 0.8395\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 1.5823 Acc: 0.8680\n",
      "val Loss: 1.6024 Acc: 0.8345\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 1.5798 Acc: 0.8682\n",
      "val Loss: 1.6017 Acc: 0.8320\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 1.5775 Acc: 0.8686\n",
      "val Loss: 1.5991 Acc: 0.8420\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 1.5752 Acc: 0.8705\n",
      "val Loss: 1.5970 Acc: 0.8355\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 1.5730 Acc: 0.8689\n",
      "val Loss: 1.5963 Acc: 0.8385\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 1.5715 Acc: 0.8681\n",
      "val Loss: 1.5943 Acc: 0.8370\n",
      "\n"
     ]
    }
   ],
   "source": [
    "function = \"sigmoid\"\n",
    "\n",
    "print(\"Dataset: MNIST\")\n",
    "print(\"Activation function:\", function)\n",
    "\n",
    "model = ModelMNIST(function)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "model, accs[function] = train(model, mnist_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: MNIST\n",
      "Activation function: tanh\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 1.5663 Acc: 0.6813\n",
      "val Loss: 1.1392 Acc: 0.8800\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 1.0692 Acc: 0.8949\n",
      "val Loss: 1.0308 Acc: 0.9080\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.9906 Acc: 0.9213\n",
      "val Loss: 0.9862 Acc: 0.9185\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.9530 Acc: 0.9350\n",
      "val Loss: 0.9587 Acc: 0.9265\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.9320 Acc: 0.9401\n",
      "val Loss: 0.9462 Acc: 0.9280\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.9138 Acc: 0.9474\n",
      "val Loss: 0.9387 Acc: 0.9280\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.9009 Acc: 0.9536\n",
      "val Loss: 0.9256 Acc: 0.9360\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.8869 Acc: 0.9584\n",
      "val Loss: 0.9191 Acc: 0.9405\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.8793 Acc: 0.9627\n",
      "val Loss: 0.9192 Acc: 0.9390\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.8713 Acc: 0.9654\n",
      "val Loss: 0.9116 Acc: 0.9430\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.8637 Acc: 0.9699\n",
      "val Loss: 0.9112 Acc: 0.9400\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.8575 Acc: 0.9721\n",
      "val Loss: 0.9099 Acc: 0.9405\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.8527 Acc: 0.9761\n",
      "val Loss: 0.9052 Acc: 0.9455\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.8497 Acc: 0.9759\n",
      "val Loss: 0.9017 Acc: 0.9420\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.8452 Acc: 0.9776\n",
      "val Loss: 0.9019 Acc: 0.9460\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.8404 Acc: 0.9822\n",
      "val Loss: 0.9006 Acc: 0.9430\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.8363 Acc: 0.9811\n",
      "val Loss: 0.9025 Acc: 0.9425\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.8349 Acc: 0.9832\n",
      "val Loss: 0.9005 Acc: 0.9455\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.8323 Acc: 0.9840\n",
      "val Loss: 0.8996 Acc: 0.9475\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.8288 Acc: 0.9859\n",
      "val Loss: 0.9005 Acc: 0.9495\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.8275 Acc: 0.9865\n",
      "val Loss: 0.8990 Acc: 0.9485\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.8252 Acc: 0.9874\n",
      "val Loss: 0.8990 Acc: 0.9500\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.8258 Acc: 0.9874\n",
      "val Loss: 0.9006 Acc: 0.9465\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.8239 Acc: 0.9881\n",
      "val Loss: 0.8989 Acc: 0.9460\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.8219 Acc: 0.9886\n",
      "val Loss: 0.9009 Acc: 0.9450\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.8210 Acc: 0.9892\n",
      "val Loss: 0.8964 Acc: 0.9480\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.8198 Acc: 0.9892\n",
      "val Loss: 0.8973 Acc: 0.9495\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.8194 Acc: 0.9895\n",
      "val Loss: 0.8989 Acc: 0.9485\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.8202 Acc: 0.9889\n",
      "val Loss: 0.8961 Acc: 0.9465\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.8195 Acc: 0.9894\n",
      "val Loss: 0.8962 Acc: 0.9510\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.8175 Acc: 0.9901\n",
      "val Loss: 0.8961 Acc: 0.9495\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.8159 Acc: 0.9901\n",
      "val Loss: 0.8969 Acc: 0.9475\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.8145 Acc: 0.9910\n",
      "val Loss: 0.8935 Acc: 0.9525\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.8138 Acc: 0.9911\n",
      "val Loss: 0.8938 Acc: 0.9520\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.8139 Acc: 0.9910\n",
      "val Loss: 0.8959 Acc: 0.9510\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.8128 Acc: 0.9910\n",
      "val Loss: 0.8955 Acc: 0.9515\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.8124 Acc: 0.9915\n",
      "val Loss: 0.8957 Acc: 0.9505\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.8120 Acc: 0.9919\n",
      "val Loss: 0.8955 Acc: 0.9525\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.8117 Acc: 0.9916\n",
      "val Loss: 0.8959 Acc: 0.9520\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.8114 Acc: 0.9920\n",
      "val Loss: 0.8957 Acc: 0.9515\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.8113 Acc: 0.9918\n",
      "val Loss: 0.8962 Acc: 0.9515\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.8113 Acc: 0.9919\n",
      "val Loss: 0.8955 Acc: 0.9520\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.8109 Acc: 0.9921\n",
      "val Loss: 0.8953 Acc: 0.9510\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.8105 Acc: 0.9925\n",
      "val Loss: 0.8963 Acc: 0.9520\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.8102 Acc: 0.9930\n",
      "val Loss: 0.8984 Acc: 0.9495\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.8099 Acc: 0.9929\n",
      "val Loss: 0.8977 Acc: 0.9505\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.8097 Acc: 0.9930\n",
      "val Loss: 0.8977 Acc: 0.9510\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.8095 Acc: 0.9930\n",
      "val Loss: 0.8967 Acc: 0.9510\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.8093 Acc: 0.9930\n",
      "val Loss: 0.8973 Acc: 0.9495\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.8090 Acc: 0.9930\n",
      "val Loss: 0.8970 Acc: 0.9475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "function = \"tanh\"\n",
    "\n",
    "print(\"Dataset: MNIST\")\n",
    "print(\"Activation function:\", function)\n",
    "\n",
    "model = ModelMNIST(function)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "model, accs[function] = train(model, mnist_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: MNIST\n",
      "Activation function: relu\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 1.6533 Acc: 0.4230\n",
      "val Loss: 0.3923 Acc: 0.8795\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.3269 Acc: 0.9036\n",
      "val Loss: 0.2867 Acc: 0.9150\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.2107 Acc: 0.9334\n",
      "val Loss: 0.1923 Acc: 0.9445\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.1584 Acc: 0.9497\n",
      "val Loss: 0.1946 Acc: 0.9405\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.1227 Acc: 0.9621\n",
      "val Loss: 0.1642 Acc: 0.9475\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.1022 Acc: 0.9656\n",
      "val Loss: 0.1595 Acc: 0.9475\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.0829 Acc: 0.9710\n",
      "val Loss: 0.1878 Acc: 0.9455\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.0620 Acc: 0.9794\n",
      "val Loss: 0.1490 Acc: 0.9545\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.0527 Acc: 0.9815\n",
      "val Loss: 0.2106 Acc: 0.9465\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.0371 Acc: 0.9876\n",
      "val Loss: 0.1784 Acc: 0.9540\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.0323 Acc: 0.9895\n",
      "val Loss: 0.1924 Acc: 0.9575\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.0353 Acc: 0.9885\n",
      "val Loss: 0.1727 Acc: 0.9530\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 0.0320 Acc: 0.9900\n",
      "val Loss: 0.1750 Acc: 0.9555\n",
      "\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 0.0294 Acc: 0.9912\n",
      "val Loss: 0.1883 Acc: 0.9570\n",
      "\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 0.0331 Acc: 0.9895\n",
      "val Loss: 0.1783 Acc: 0.9545\n",
      "\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 0.0248 Acc: 0.9910\n",
      "val Loss: 0.1815 Acc: 0.9585\n",
      "\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 0.0112 Acc: 0.9962\n",
      "val Loss: 0.2235 Acc: 0.9530\n",
      "\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 0.0204 Acc: 0.9928\n",
      "val Loss: 0.1959 Acc: 0.9560\n",
      "\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 0.0175 Acc: 0.9938\n",
      "val Loss: 0.1720 Acc: 0.9600\n",
      "\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 0.0124 Acc: 0.9955\n",
      "val Loss: 0.1984 Acc: 0.9560\n",
      "\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 0.0108 Acc: 0.9971\n",
      "val Loss: 0.2152 Acc: 0.9575\n",
      "\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 0.0059 Acc: 0.9984\n",
      "val Loss: 0.2124 Acc: 0.9615\n",
      "\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 0.0024 Acc: 0.9992\n",
      "val Loss: 0.1992 Acc: 0.9630\n",
      "\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 0.9999\n",
      "val Loss: 0.2010 Acc: 0.9630\n",
      "\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.2049 Acc: 0.9635\n",
      "\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.2096 Acc: 0.9640\n",
      "\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.2133 Acc: 0.9640\n",
      "\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.2163 Acc: 0.9640\n",
      "\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.2191 Acc: 0.9640\n",
      "\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9999\n",
      "val Loss: 0.2214 Acc: 0.9640\n",
      "\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2236 Acc: 0.9635\n",
      "\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2256 Acc: 0.9635\n",
      "\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2273 Acc: 0.9635\n",
      "\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2291 Acc: 0.9635\n",
      "\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2307 Acc: 0.9635\n",
      "\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2320 Acc: 0.9635\n",
      "\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2334 Acc: 0.9635\n",
      "\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2347 Acc: 0.9640\n",
      "\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2359 Acc: 0.9640\n",
      "\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2370 Acc: 0.9640\n",
      "\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2380 Acc: 0.9640\n",
      "\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2391 Acc: 0.9640\n",
      "\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2402 Acc: 0.9640\n",
      "\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2412 Acc: 0.9640\n",
      "\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2420 Acc: 0.9640\n",
      "\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2430 Acc: 0.9640\n",
      "\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2439 Acc: 0.9640\n",
      "\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2447 Acc: 0.9640\n",
      "\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2455 Acc: 0.9640\n",
      "\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9999\n",
      "val Loss: 0.2463 Acc: 0.9640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "function = \"relu\"\n",
    "\n",
    "print(\"Dataset: MNIST\")\n",
    "print(\"Activation function:\", function)\n",
    "\n",
    "model = ModelMNIST(function)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "model, accs[function] = train(model, mnist_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xs = list(range(50))\n",
    "plot1, = plt.plot(xs, accs[\"sigmoid\"])\n",
    "plot2, = plt.plot(xs, accs[\"tanh\"])\n",
    "plot3, = plt.plot(xs, accs[\"relu\"])\n",
    "\n",
    "plt.title(\"MNIST functions comparation\")\n",
    "plt.legend([plot1, plot2, plot3], [\"Sigmoid\", \"Tanh\", \"ReLU\"])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"val accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.4840 Acc: 0.8657\n",
      "val Loss: 0.2703 Acc: 0.9196\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.2665 Acc: 0.9241\n",
      "val Loss: 0.2227 Acc: 0.9356\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.2181 Acc: 0.9383\n",
      "val Loss: 0.2005 Acc: 0.9407\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.2188 Acc: 0.9355\n",
      "val Loss: 0.2221 Acc: 0.9279\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.1523 Acc: 0.9543\n",
      "val Loss: 0.2542 Acc: 0.9244\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.2194 Acc: 0.9368\n",
      "val Loss: 0.1626 Acc: 0.9485\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.1246 Acc: 0.9615\n",
      "val Loss: 0.1754 Acc: 0.9487\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.1017 Acc: 0.9684\n",
      "val Loss: 0.1663 Acc: 0.9503\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.0985 Acc: 0.9696\n",
      "val Loss: 0.1718 Acc: 0.9501\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.0836 Acc: 0.9723\n",
      "val Loss: 0.1663 Acc: 0.9551\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1034 Acc: 0.9700\n",
      "val Loss: 0.1851 Acc: 0.9399\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.0716 Acc: 0.9774\n",
      "val Loss: 0.1608 Acc: 0.9554\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.0764 Acc: 0.9786\n",
      "val Loss: 0.1631 Acc: 0.9546\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.0728 Acc: 0.9771\n",
      "val Loss: 0.1777 Acc: 0.9509\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.0562 Acc: 0.9830\n",
      "val Loss: 0.1750 Acc: 0.9554\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.0485 Acc: 0.9840\n",
      "val Loss: 0.2100 Acc: 0.9458\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.0480 Acc: 0.9866\n",
      "val Loss: 0.1837 Acc: 0.9562\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.0451 Acc: 0.9868\n",
      "val Loss: 0.1855 Acc: 0.9533\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.0344 Acc: 0.9900\n",
      "val Loss: 0.1894 Acc: 0.9525\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.0293 Acc: 0.9904\n",
      "val Loss: 0.2015 Acc: 0.9476\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.0233 Acc: 0.9925\n",
      "val Loss: 0.2361 Acc: 0.9482\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.0580 Acc: 0.9852\n",
      "val Loss: 0.2555 Acc: 0.9319\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.0931 Acc: 0.9758\n",
      "val Loss: 0.1871 Acc: 0.9514\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.0277 Acc: 0.9911\n",
      "val Loss: 0.1808 Acc: 0.9583\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.0186 Acc: 0.9949\n",
      "val Loss: 0.1896 Acc: 0.9557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_collate_fn(add_noise=False):\n",
    "    def collate_fn(data):\n",
    "        xs, ys = zip(*data)\n",
    "        xs = torch.stack(xs, 0)\n",
    "        ys = torch.stack(ys, 0)\n",
    "        if add_noise:\n",
    "            xs += torch.randn_like(xs) / 10\n",
    "        return xs, ys\n",
    "    return collate_fn\n",
    "\n",
    "not_mnist_loader = {x: data.DataLoader(not_mnist[x], \n",
    "                                       batch_size=32,\n",
    "                                       shuffle=(x == \"train\"), \n",
    "                                       num_workers=4,\n",
    "                                       collate_fn=get_collate_fn(x == \"train\"))\n",
    "                    for x in ['train', 'val']}\n",
    "\n",
    "model = models.resnet34(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "model, notMNIST_accs = train(model, not_mnist_loader, criterion, optimizer, device, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max accuracy for notMNIST dataset: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "print(\"Max accuracy for notMNIST dataset:\", max(notMNIST_accs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
